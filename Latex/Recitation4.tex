\documentclass{beamer}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usetheme{Madrid}
\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}
\usepackage{outlines}
\usepackage{enumitem}
\renewenvironment{itemize}
\usefonttheme{serif}
\def\labelenumi{\theenumi}
\usefonttheme{serif}

%\setbeamertemplate{itemize items}[default]
%\setbeamertemplate{enumerate items}[default]

% define smaller font command
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand\Fonteight{\fontsize{8}{9.6}\selectfont}
%define enumerate with periods
\renewenvironment{enumerate}%
{\begin{list}{\arabic{enumi}.}% <------ dot here
      {\setlength{\leftmargin}{2.5em}%
       \setlength{\itemsep}{-\parsep}%
       \setlength{\topsep}{-\parskip}%%
       \usecounter{enumi}}%
 }{\end{list}}
%define itemize with arrows
\renewenvironment{itemize}%
{\begin{list}{$\blacktriangleright$}% <------ dot here
      {\setlength{\leftmargin}{2.5em}%
       \setlength{\itemsep}{-\parsep}%
       \setlength{\topsep}{-\parskip}%%
       \usecounter{enumi}}%
 }{\end{list}}

%Information to be included in the title page:
\title{Recitation 3}
\author{Alex Dong}
\institute{CDS, NYU}
\date{Fall 2020}


\makeatletter
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.4\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor\expandafter\ifblank\expandafter{\beamer@shortinstitute}{}{~~(\insertshortinstitute)}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
    \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
  \end{beamercolorbox}}%
  \vskip0pt%
}
\setbeamertemplate{navigation symbols}{}
\makeatother

\begin{document}
%1
\frame{\titlepage} 
%2

\begin{frame}
\frametitle{Norms}
Norms measure distances!
Think about all the properties of distance that make sense.
\begin{itemize}
\item distance = 0 means at the same point
\item distance is always non-negative
\item distance follows triangle inequality (well... at least in euclidean space)
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Norms}
Shorthand way to remember what the properties do.
\begin{definition}[Norm]
	A norm $\| \cdot \|$ on $V$ verifies the following points:
	\begin{enumerate}
		\item \emph{Triangular inequality}: $\|u + v\| \leq \|u\| + \|v\|$ \hfill "Euclidean space"
		\item \emph{Homogeneity}: $\| \alpha v \| = |\alpha|\times \| v\|$ \hfill "farther actually means farther"
		\item \emph{Positive definiteness}: if $\|v\| = 0 \implies v=0$. \hfill "Non-negative"
	\end{enumerate}
\end{definition}
\end{frame}

\begin{frame}
\frametitle{Inner Products}
Inner products measure angles! 
... but not directly.
$$cos(\theta) = \frac{<u,v>}{\|u\| \|v\|}$$
or
$$<u,v>\ =\ cos(\theta) \|u\|\|v\|$$
When $u,v$ are unit vectors, inner product gives a measure for angle between vectors.
\end{frame}

\begin{frame}
\frametitle{Inner Products in Machine Learning}
\begin{itemize}
\item Angles can be used as a measure of similarity
\item Kernel Tricks - Increase data complexity
\begin{itemize}
\item Sometimes you have to calculate  $x_{old}^T x_{new}$, equivalently $<x_{old},x_{new}>$
\item You can replace the inner product with a inner product in a higher dimensional space
\item Instead of calculating $<x_{old},x_{new}>$, define a function $K$ and calculate $<K(x_{old}),K(x_{new})>$
\item if you pick ''the right" higher dimensional space, your data can be a lot easier to work with
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Orthogonality}
\begin{itemize}
\item Angles can be used as a measure of similarity
\item Vectors $u,v$ are orthogonal if and only if $<u,v>=0$
\item Vectors are orthogonal $\implies$ vectors are as dissimilar as possible
\item Orthogonal coordinate systems are good because we can view each coordinate independently.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Orthogonal Projections}
\begin{itemize}
\item Projections form an important part of linear algebra.
\item One of the key ideas in linear algebra is to decompose a matrix.
Specifically, to build the original matrix by using smaller, \textit{easier to understand} pieces.
By examining these \textit{simpler} pieces, we can understand what the matrix does. We can even tinker with them.
\end{itemize}
Projections
\end{frame}



\begin{frame}
\frametitle{Idempotence}
In class, we saw any orthogonal projection $P_S$ takes the form
$$P_S = VV^T$$
where V is a matrix of orthonormal columns that form a basis for $S$.\\

There is a more general definition of a projection - known as \textit{idempotence}.
\begin{definition}[Idempotence]
An matrix P is idempotent when
	$$
	P^2=P
	$$
\end{definition}


\end{frame}

\begin{frame}
\frametitle{Orthgonal projections vs Idempotence}
\begin{definition}[Idempotence]
An matrix P is idempotent when
	\begin{center}
	$P^2=P$
	\end{center}
\end{definition}


\begin{enumerate}
\item Show that all orthogonal projections are idempotent.
\item Give an example of an idempotent matrix that is not an orthogonal projection.  \\
(Hint: Show that your matrix does not minimize the distance to subspace it projects onto.)
\end{enumerate}
\end{frame}


\begin{frame}

\frametitle{Questions: Orthogonal Projections}
\begin{enumerate}
\item Let $v_1,...,v_k$ be a list of orthogonal vectors. Show that $v_1,...,v_k$ are linearly independent.
\item Let U be the subspace of $\R^n$ with orthonormal basis $u_1,...,u_k$.
\begin{enumerate}
 \item Prove that the orthogonal projection of $v \in \R^n$ onto $U$ can be expressed as $P_U  = \sum_{i=0}^k <v,u_i>$. (Use the fact that the orthonormal basis for a subspace of $\R$ can be extended to obtain an orthonormal basis for $\R$)
 \item Prove that $P_U(v)\leq \|v\|$
 \item Prove that $v-P_U(v)$ is orthogonal to $P_U(v)$
\end{enumerate}
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Questions: Norms and Inner Products}

\begin{enumerate}
\item Which of the following functions are inner products for $x,y\in\R^3$?
\begin{enumerate}
\item[i.] $f(x,y) = x_1y_2+x_2y_3+x_3y_1$
\item[ii.] $f(x,y) = x_1^2y_1^2+x_2^2y_2^2+x_1^2y_1^2$
\item[iii.] $f(x,y) = x_1y_1+x_3y_3$
\end{enumerate}
\item For $A \in \R^{m\times n}$ and $x\in \R^n$, prove that
$$ \|Ax\| \leq \|x\|\sqrt{\sum_{i=1}^m \sum_{j=1}^n} A_{i,j}^2$$


\end{enumerate}

\end{frame}




\end{document}
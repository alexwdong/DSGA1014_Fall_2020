\documentclass[table]{beamer}
\usepackage{xcolor}

\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usetheme{Madrid}
\usepackage{../latex_style/packages_old}
\usepackage{../latex_style/notations_old}
\usepackage{outlines}
\usepackage{enumitem}
\renewenvironment{itemize}
\usefonttheme{serif}
\def\labelenumi{\theenumi}
\usefonttheme{serif}

%\setbeamertemplate{itemize items}[default]
%\setbeamertemplate{enumerate items}[default]

% define smaller font command
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand\fonteight{\fontsize{8}{9.6}\selectfont}
\newcommand\fontten{\fontsize{10}{1.2}\selectfont}
\newcommand\fonttwelve{\fontsize{12}{1.44}\selectfont}

%define enumerate with periods
\renewenvironment{enumerate}%
{\begin{list}{\arabic{enumi}.}%  \langle ------ dot here
      {\setlength{\leftmargin}{2.5em}%
       \setlength{\itemsep}{-\parsep}%
       \setlength{\topsep}{-\parskip}%%
       \usecounter{enumi}}%
 }{\end{list}}
%define itemize with arrows
\renewenvironment{itemize}%
{\begin{list}{$\blacktriangleright$}%  \langle ------ dot here
      {\setlength{\leftmargin}{2.5em}%
       \setlength{\itemsep}{-\parsep}%
       \setlength{\topsep}{-\parskip}%%
       \usecounter{enumi}}%
 }{\end{list}}

%Information to be included in the title page:
\title{Recitation 2}
\author{Alex Dong}
\institute{CDS, NYU}
\date{Fall 2020}


\makeatletter
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.4\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor\expandafter\ifblank\expandafter{\beamer@shortinstitute}{}{~~(\insertshortinstitute)}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
    \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
  \end{beamercolorbox}}%
  \vskip0pt%
}
\setbeamertemplate{navigation symbols}{}
\makeatother

\begin{document}
%1
\frame{\titlepage} 

%2
\begin{frame}
\frametitle{Concept Review: Orthogonal Matrices}
\begin{itemize}
\item \textbf{Orthogonal} matrices have \textit{orthonormal} columns
\begin{itemize}
\item Stronger condition than having orthogonal columns
\item Bad terminology thats grandfathered in
\end{itemize}
\item We will see a lot of these matrices
\item Orthogonal matrices preserve angles and norms
\begin{itemize}
\item This leads to a very natural \textit{change of basis} - more later
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Questions: Orthogonal Matrices}
\begin{enumerate}
\item Let $Q,U \in \R^{n \times n}$ be an orthogonal matrix. Let $x,y \in \R^n$.
\begin{enumerate}
\item[i.] Show that $\langle Qx, Qy \rangle = \langle x, y \rangle.$ 
\item[ii.] Show that $\|Qx\| = \|x\|.$
\item[iii.] Show that $QU$ is orthogonal.
\end{enumerate}
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Solutions 1: Orthogonal Matrices}
\begin{enumerate}
\item Let $Q,U \in \R^{n \times n}$ be an orthogonal matrix. Let $x,y \in \R^n$.
\begin{enumerate}
\item[ii.]  Show that $\langle Qx, Qy \rangle = \langle x, y \rangle.$
\begin{solution}
$\langle Qx, Qy \rangle = x^TQ^TQy = x^T I y = x^Ty = \langle x, y \rangle$

\end{solution}
\end{enumerate}
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Solutions 2: Orthogonal Matrices}
\begin{enumerate}
\item Let $Q,U \in \R^{n \times n}$ be an orthogonal matrix. Let $x,y \in \R^n$.
\begin{enumerate}
\item[ii.] Show that $\|Qx\| = \|x\|.$\\
\begin{solution}
$\|Qx\| = \langle Qx, Qx \rangle = x^TQ^TQx = x^T I x = x^Tx = \langle x, x \rangle = \|x\|$
\end{solution}
\end{enumerate}
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{Solutions 3: Orthogonal Matrices}
\begin{enumerate}
\item Let $Q,U \in \R^{n \times n}$ be an orthogonal matrix. Let $x,y \in \R^n$.
\begin{enumerate}
\item[iii.] Show that $QU$ is orthogonal.
\begin{solution}
Start by noticing that $(QU)^T = U^TQ^T.$ \\
Now,\\
\qquad $(QU)^T(QU) = U^TQ^TQU = U^T I U = I$\\
and \\
\qquad $(QU)(QU)^T = QUU^TQ^T = Q I Q^T = I$ \\
Then $QU$ is orthogonal.
\end{solution}
\end{enumerate}
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Concept Review: Gram-Schmidt Process}
\begin{itemize}
\item Gram-Schmidt Process turns a basis of linearly independent vectors into orthonormal vectors
\item Understanding the GS process is important, but we will mainly only use its existence
\begin{itemize}
\item Let $v_1,...,v_n$ be a basis ... \\
... and by GS process, let $u_1,...,u_n$ be orthonormal with $Span(v_1,...,v_n)=Span(u_1,...,u_n)$:
\item Let $u_1,...,u_n$ be an orthonormal basis of $\R^n$
\end{itemize}

\item Related to QR Factorization
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Questions: GS Process and QR Factorization}
\begin{enumerate}
\item Let $A\in \R^{n\times n}$ have linearly independent columns. Show that there is a matrix $Q\in \R^{n \times m}$ and $R \in \R^{n\times n} $ s.t that $A=QR$, where $Q$ has orthonormal columns and $R$ is upper triangular. \\
\medskip
(Hint: Recall the ``linear combination of columns interpretation of matrix multiplication").
\end{enumerate}
\end{frame}

\begin{frame}

\begin{enumerate}
\frametitle{Solutions: GS Process and QR Factorization}

\item Let $A\in \R^{m\times n}$ have linearly independent columns. Show that there is a matrix $Q\in \R^{m\times m}$ and $R \in \R^{m\times n} $ s.t that $A=QR$, where $Q$ has orthonormal columns and $R$ is upper triangular. \\
\begin{solution}
First, let $v_1,...,v_n$ be the columns of $A$.\\
 Apply the GS process to get $u_1,...,u_n$. \\
 Now, let $Q$ have $u_1,...,u_n$ as its columns. \\
 Note that by the GS process, we have $Span(v_1,..., v_i) = Span(u_1,...,u_i)\ \forall i\in \{1,...,n\}$.\\
  Then each column $v_i$ is a linear combination of the columns $u_1,...,u_i$. \\
  Then this exactly saying that $A=QR$, where $R$ contains the coefficients that transforms $u_1,...,u_i$ into $v_1,...,v_i\ \forall i \in \{1,...,n\}$!
\end{solution}
\end{enumerate}
(!Check the next slide!)
\end{frame}


\begin{frame}
\frametitle{More M.M.M: Linear Combination of Columns}
(From Recitation 2) \\ 
Each column of the $AB$ is a linear combination of the columns of $A$.\\
\medskip
%{>{\columncolor{red!20}}c>{\columncolor{blue!20}}c>{\columncolor{green!20}}cc >{\columncolor{yellow!20}}c}
$
\left[\begin{array}{>{\columncolor{red!20}}c>{\columncolor{blue!20}}cc>{\columncolor{green!20}}c >{\columncolor{yellow!20}}c}
\vline & \vline & \hdots & \vline  & \vline \\ 
\mathbf{a_1}    & \mathbf{a_2}    & \hdots & \mathbf{a_{k-1}}   &  \mathbf{a_k} \\
\vline & \vline & \hdots & \vline  & \vline \\ 
\end{array}\right]
$
$
\left[\begin{matrix}
b_{1,1}    & \hdots &   {\cellcolor{red!20}}  b_{1,m}  \\ 
b_{2,1}    & \hdots &   {\cellcolor{blue!20}}   b_{2,m}  \\ 
  \vdots   & \vdots &   \vdots \\
b_{k-1,1}  & \hdots &   {\cellcolor{green!20}} b_{k-1,m}  \\ 
b_{k,1}    & \hdots &   {\cellcolor{yellow!20}}b_{k,m}
\end{matrix}\right]
$\\
\medskip
$=
\left[\begin{array}{cc>{\columncolor{gray!20}}c}
\vline  & \hdots   & \vline \\ 
\sum_{i=1}^k \mathbf{a_i} b_{i,1}   & \hdots &  \sum_{i=1}^k \mathbf{a_i} b_{i,m} \\
\vline  & \hdots  & \vline \\ 
\end{array}\right]
$

\end{frame}

\begin{frame}
\frametitle{A Note About Determinants}
\begin{itemize}
\item Eigenvalues of a matrix can be determined by using determinants
\item \textbf{Not covered in this course!}
\begin{itemize}
\item ``too long to define, a bit complex, and slightly useless in data science..." - L\'eo
\end{itemize}
\item  Determinants lead to a lot of cool things
\begin{itemize}
\item Trace(A) = sum of eigenvalues of A (with multiplicity)
\item (\&) A matrix satisfies it's own \textit{characteristic polynomial} - Cayley Hamilton Theorem
\item (\&) Matrix polynomial rabbit hole runs deep (Jordan Normal Form)
\end{itemize}
\item Interesting from a pure math perspective
\end{itemize}

\footnotetext{(\&) denotes extra material not covered in this course}
\end{frame}

\begin{frame}
\frametitle{Etymology}

\begin{itemize}
\item \textit{eigen}values and \textit{eigen}vectors
\item What does \textit{eigen} mean anyway?
\item German word for...
\begin{enumerate}
\item own
\item innate
\item peculiar
\item \textbf{intrinsic}
\end{enumerate}
\item A square matrix `owns' certain vectors... or there are certain vectors that are intrinsic to a matrix.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Importance of Eigenvalues and Eigenvectors}
\center{!!! \textit{\textbf{SERIOUSLY IMPORTANT}} !!!}\\
\bigskip
\begin{itemize}
\item Eigen-val/vec will show up \textit{continuously} throughout this course
\item Connections to...
\begin{itemize}
\item Projections and Orthogonal Projections (Lec 4)
\item Markov Chains (Lec 6)
\item Spectral Theorem (HW 6, Lec 7)
\item SVD (Lec 7)
\item Spectral Clustering (!!??) (Lec 8)
\item Positive definite and positive semi-definite matrices (Lec 10,11)
\end{itemize}
\item Many other applications not covered in this course
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{$Av=\lambda v$. So what's the big deal?}
\begin{itemize}
\item Square matrices get their own name - \textit{operators}.
\begin{itemize}
\item Can construct powers of operators ($A^k$)
\item Operators can be invertible
\item Operators can be symmetric
\end{itemize}
\item Sometimes an matrix A `prefers' certain directions
\item (!!!) These directions act as \textit{anchors} for understanding the action of a matrix.
\item We will see how to exploit these directions in order to simplify our understanding of matrices. (Diagonalizability, Lec 7)
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Questions: Eigen}
Let $A \in \R^{n\times n}$ have eigenvalue $\lambda$ associated to eigenvector $v$.
Show that:
\begin{enumerate}
\item $\forall \alpha \in \R$, $\lambda+\alpha$ is an eigenvalue of $A+\alpha I$ w/ eigenvector $v$.
\item $\forall k \in \N$, $\lambda^k$ is an eigenvalue of $A^k$ w/ eigenvector $v$.
\item Let $A\in \R^{n \times n}$ have eigenvalue-vector pairs $\lambda_1,...,\lambda_n$ and $v_1,...,v_n$. \\
Also, assume that $\lambda_1>...>\lambda_n$.\\
Prove that $v_1,...,v_n$ are linearly independent.\\

\medskip
Hint: First assume transform all $\lambda_i$ to be positive.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Solutions 1: Eigen}
Let $A \in \R^{n\times n}$ have eigenvalue $\lambda$ associated to eigenvector $v$.
Show that:
\begin{enumerate}
\item $\forall \alpha \in \R$, $\lambda+\alpha$ is an eigenvalue of $A+\alpha I$ w/ eigenvector $v$.
\begin{solution}
Let $\alpha\in \R$, and $v$ be an eigenvector of A.\\
Consider the matrix $A+\alpha I$.\\
\begin{align*}
(A+\alpha I)v &= Av+\alpha Iv \\
&= \lambda v+\alpha v\\
&= (\lambda+\alpha)v \\
\end{align*}
So $\lambda+\alpha$ is an eigenvalue of $A+\alpha I$ with eigenvector $v$.
\end{solution}
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Solutions 2: Eigen}
Let $A \in \R^{n\times n}$ have eigenvalue $\lambda$ associated to eigenvector $v$.
Show that:
\begin{enumerate}
\item[2.] $\forall k \in \N$, $\lambda^k$ is an eigenvalue of $A^k$ w/ eigenvector $v$.
\begin{solution}
Let $k \in \N$, and $v$ be an eigenvector of $A$.\\
Consider the matrix $A^k$.\\
\begin{align*}
A^k v &= A... Av \qquad \text{k times} \\
A^k v &= A... A(\lambda v) \qquad \text{k-1 times} \\
A^k v &= \lambda^k v \\
\end{align*}
So $\lambda^k$ is an eigenvalue of $A^k$ with eigenvector $v$.
\end{solution}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Solutions 3: Eigen}

\begin{enumerate}

\item[3.] Let $A\in \R^{n \times n}$ have eigenvalue-vector pairs $\lambda_1,...,\lambda_n$ and $v_1,...,v_n$. \\
Also, assume that $\lambda_1>...>\lambda_n$.\\
Prove that $v_1,...,v_n$ are linearly independent.
\begin{solution}
\fontten
Let $B = A + |\lambda_n|I$. (This is so all eigenvalues of $B$ are $\geq 0$.) \\
Let $\gamma_i = \lambda_i+|\lambda_n|$
(Problem 1, eigenvecs of $B$ are also eigenvecs of $A$). \\
Let $\alpha_1,...,\alpha_n \in \R$ s.t $\sum_{i=1}^n \alpha_i v_i = 0$. We will show that all $\alpha_i=0$.\\
Consider $0 = B^k (\sum_{i=1}^n \alpha_i v_i)$. \\
\qquad $0 = B^k (\sum_{i=1}^n \alpha_i v_i)$ \\
\qquad $0 = \sum_{i=1}^n B^k \alpha_i v_i$ \\
\qquad $0 = \sum_{i=1}^n \gamma_i^k \alpha_i v_i$ \\ 
\qquad $0 = \gamma_1^k  \sum_{i=1}^n (\frac{\gamma_i}{\gamma_1})^k \alpha_i v_i$  \\ 
\qquad $0 = \lim_{k \rightarrow \infty} \gamma_1^k  \sum_{i=1}^n (\frac{\gamma_i}{\gamma_1})^k \alpha_i v_i$ \\
\qquad $0 = \alpha_1v_1$ \quad since $\frac{\gamma_i}{\gamma_1} < 1$ for $ i \neq 1$ \\
Then $0 = (\sum_{\mathbf{i=2}}^n \alpha_i v_i)$. Repeat the previous logic to find that each $\alpha_iv_i=0$. Then all $\alpha_i=0$. So $v_1,...,v_n$ are linearly independent. 
\end{solution}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Questions 2: Properties of Orthogonal Matrices}
Let $Q\in \R^{n\times n}$ be orthogonal.
\begin{enumerate}
\item Does $Q$ necessarily have eigenvalues and eigenvectors?
\bigskip
\end{enumerate}
Assume that $Q$ has eigenvalues $\lambda_1,...,\lambda_k$.
\begin{enumerate}
\item[2.] Describe the eigenvalues of $Q$.
\end{enumerate}

\end{frame}
\begin{frame}
\frametitle{Solutions 2: Properties of Orthogonal Matrices}
Let $Q\in \R^{n\times n}$ be orthogonal.
\begin{enumerate}
\item Does $Q$ necessarily have eigenvalues and eigenvectors?
\begin{solution}
No, consider the matrix 
$Q=\begin{bmatrix}
0 & -1 \\
1 & 0 \\
\end{bmatrix}$
(90 deg CCW rotation in $\R^2$).
\end{solution}
\end{enumerate}
Assume that $Q$ has eigenvalues $\lambda_1,...,\lambda_k$.
\begin{enumerate}
\item[2.] Describe the eigenvalues of $Q$.
\begin{solution}
\fontten
Since Q is orthogonal then $\forall x\in \R^n$\\
\qquad $\|Qx\|= \langle Qx,Qx\rangle$ \\
\qquad $\|Qx\|= x^TQ^TQx $ \\
\qquad $\|Qx\|=  xIx $ \\ 
\qquad $\|Qx\|= \|x\|$ \\

Now, if $x$ is an eigenvector of $Q$ with eigenvalue $\lambda$, then we have \\
\qquad $\|x\| = \|Qx\| = \|\lambda x\| = |\lambda| \|x\|$. So $\lambda = \pm 1 $. \\
\end{solution}
\end{enumerate}

\end{frame}


\end{document}
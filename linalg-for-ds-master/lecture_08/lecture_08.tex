\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}
%\externaldocument{../lecture_02/lecture_02}
\externaldocument{../lecture_07/lecture_07}


\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Lecture 8: Graphs and Linear Algebra}
\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\input{../preamble.tex}

\section{Graphs}

We start by a formal definition of a (simple non-oriented) graph:
\begin{definition}[Graph]
	A graph $G$ is defined as a pair $V_G,E_G$ where $V=V_G$ is the set of vertices of $G$ and $E = E_G$ is the set of edges of $G$ which is a subset of $V \times V$.
	Two vertices $i,j$ are connected by an edge if $\{i,j\} \in E$. In such case we write $i \sim j$ and say that $i$ and $j$ are neighbors.
\end{definition}

We say that $G$ is connected if it is possible to go from every vertex in the graph to every other vertex through a series of edges, called a path.

\begin{definition}
	The degree $\deg(i)$ of a node $i \in V$ is the number of its neighbors.
\end{definition}

In this lecture we will only consider finite graphs, where $V$ is finite. We let $n = \# V$. One can assume (up to renaming the vertices) that $V = \{1, \dots, n\}$.

\begin{definition}
	We define the adjacency matrix $A \in \R^{n \times n}$ of the graph $G$ by
	$$
	A_{i,j} = 
	\begin{cases}
		1 & \text{if} \ i \sim j \\
		0 & \text{otherwise.}
	\end{cases}
	$$
	The degree matrix of $G$ is defined by $D = \Diag(\deg(1), \dots, \deg(n))$. 
\end{definition}

Notice that $A$ is a symmetric matrix.

\section{Graph Laplacian}

\begin{definition}[Graph Laplacian]
	The Laplacian matrix of $G$ is defined as
	$$
	L = D - A.
	$$
\end{definition}

\begin{remark}
	There exists a ``normalized Laplacian'' matrix $L_{\rm norm} = D^{-1/2} L D^{-1/2} = \Id - D^{-1/2} A D^{-1/2}$. Both $L$ and $L_{\rm norm}$ enjoy similar properties.
	While we focus for simplicity on $L$ in this lecture, several arguments advocate for using $L_{\rm norm}$ instead of $L$ for clustering applications, see the discussion in \cite[Section~8.4]{von2007tutorial}.
\end{remark}

\begin{proposition}
	The matrix $L$ satisfies the following properties:
	\begin{enumerate}
		\item $L$ is symmetric and positive semi-definite.
		\item The smallest eigenvalue of $L$ is $0$, the corresponding eigenvector is the constant one vector $\1 \defeq (1,1, \dots, 1)$.
		\item $L$ has $n$ non-negative eigenvalues $0 = \lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_n$.
	\end{enumerate}
\end{proposition}

The proposition above follows from the following key identity: for all $x \in \R^n$,
\begin{equation}\label{eq:lap}
x^{\sT} L x = \sum_{i \sim j} (x_i - x_j)^2,
\end{equation}
where the sum is over all the edges $i \sim j$ of the graph.
Consequently we have $x^{\sT} L x = 0$ if and only if all the terms of the sum \eqref{eq:lap} are equal to zero, that is $x_i = x_j$ for all $i$ and $j$ that are connected by an edge. We conclude that
\begin{center}
	\emph{
	The eigenvectors of $L$ associated with the eigenvalue $0$ are the (non-zero) vectors that remains constant on the connected components of the graph $G$, i.e.\ the vectors $x$ such that $x_i = x_j$ for all $i,j$ in the same connected component of $G$.}
\end{center}

We deduce:
\begin{proposition}\label{prop:lambda2}
	The graph $G$ is connected if and only if $\lambda_2 > 0$.
More generally, the multiplicity of the eigenvalue $0$ of $L$ (i.e.\ the number of $i$ such that $\lambda_i = 0$) is equal to the number of connected components of $L$.
\end{proposition}

The eigenvalue $\lambda_2$ is called the algebraic connectivity or the Fiedler eigenvalue of $G$. Its magnitude reflects how well connected the graph is.
Exercise: show that $\lambda_2$ increases when one adds edges to $G$.

\begin{definition}
	For a graph $G$ we define
	\begin{itemize}
		\item The vertex connectivity $v(G)$ of $G$ as the minimum number of nodes whose removal would result in losing connectivity of the graph.
		\item The edge connectivity $e(G)$ of $G$ as the minimum number of edges whose removal would result in losing connectivity of the graph.
	\end{itemize}
\end{definition}

\begin{proposition}[Fiedler \cite{fiedler1973algebraic}]\label{prop:connectivity}
	$$
	\lambda_2 \leq v(G) \leq e(G).
	$$
\end{proposition}
We will only prove the first inequality, the second if a standard fact from graph theory.
\begin{lemma}\label{lem:rem}
	Let $G^{-}$ be a graph obtained from $G$ by removing one vertex (and all the edges linked to it). Then $\lambda_2(G^{-}) \geq \lambda_2(G) - 1$.
\end{lemma}
Using Lemma \ref{lem:rem} one deduces Proposition \ref{prop:connectivity}. By definition of $v(G)$ one can disconnect $G$ by removing $v(G)$ vertices. Let $G'$ be the obtained graph. $G'$ is disconnected so $\lambda_2(G') = 0$ by Proposition \ref{prop:lambda2}. We conclude using Lemma \ref{lem:rem}:
$$
\lambda_2(G) \leq \lambda_2(G') + v(G) = v(G).
$$
\begin{proof}[of Lemma \ref{lem:rem}]
	Without loss of generalities, on can assume that the node $n$ has been removed from $G$.
	Let $G^+$ be the graph obtained from $G$ by connecting the node $n$ to all the other nodes. Its Laplacian is given by
	$$
	L_{G^+} =
	\begin{pmatrix}
		L_{G^-} + \Id & -\1 \\
		-\1^{\sT} & n-1
	\end{pmatrix}
	$$
	where $\1$ denotes the all-one vector.
	Let $x$ be an eigenvector of $L_{G^-}$ associated with $\lambda_2(G^{-})$ and orthogonal to $\1$. Compute
	$$
	L_{G^+} 
	\begin{pmatrix}
		x\\
		0
	\end{pmatrix}
	=
	\begin{pmatrix}
		L_{G^-} + \Id & -\1 \\
		-\1^{\sT} & n-1
	\end{pmatrix}
	\begin{pmatrix}
		x\\
		0
	\end{pmatrix}
	=
	\begin{pmatrix}
		L_{G^-} x + x \\
		-\1^{\sT}x 
	\end{pmatrix}
	=
(\lambda_2(G^{-}) + 1)
	\begin{pmatrix}
		 x \\
		0
	\end{pmatrix}
	$$
	because $\1^{\sT}x=0$. Hence $\lambda_2(G^{-}) + 1$ is an eigenvalue of $G^+$ different from $0$:
	$$
	\lambda_2(G^+) \leq \lambda_2(G^{-}) + 1.
	$$
Remember that we obtained $G^+$ by adding edges to $G$: $\lambda_2(G^+) \geq \lambda_2(G)$. This concludes the proof. 
\end{proof}


\section{Spectral clustering with the graph Laplacian}

From now, we assume that $G$ is connected. We aim at clustering the nodes of $G$ into $k$ groups of nodes with as many intra-group edges as possible and as few inter-group edges as possible.


\begin{algorithm}
\caption{Spectral clustering with the Laplacian}
\begin{algorithmic}[1]
	\Statex \textbf{Input:} Graph Laplacian $L$, number of clusters $k$
	\State Compute the first $k$ eigenvectors $v_1, \dots, v_k$ of the Laplacian matrix $L$.
	\State Associate to each node $i$ the vector $x_i = (v_2(i), \dots, v_k(i)) \in \R^{k-1}$.
	\State Cluster the points $x_1, \dots, x_n$ with (for instance) the $k$-means algorithm.
\end{algorithmic}
\end{algorithm}

The spectral clustering algorithm uses the eigenvectors of the Laplacian matrix to embed the nodes of the graph in the Euclidean space $\R^{k-1}$.
\\

In the sequel we will focus on the case of two clusters ($k=2$) for simplicity.
In that case, the spectral clustering algorithm amounts of computing $v_2$ the second eigenvector of $L$ (sometimes called the Fiedler eigenvector) and to cluster the nodes in
$$
S = \{ i \, | \, v_2(i) \geq \delta \} \qquad \text{and} \qquad
S^c = \{ i \, | \, v_2(i) < \delta \},
$$
for some $\delta \in \R$.
The next Proposition tells us that for $\delta = 0$, the cluster $S$ is connected.
\begin{proposition}
	Assume that $G$ is connected. Let $v_2$ be an eigenvector associated to $\lambda_2$, the second smallest eigenvalue of $L$. Let
	$$
	S = \{ i \, | \ v_2(i) \geq 0 \}.
	$$
	Then the subgraph induced by $S$ is connected.
\end{proposition}
 
See \cite{spielman2012spectral} for a proof.

\section{Spectral clustering as a relaxation}

For a set of nodes $S \subset V$ we define the cut of $S$ by:
$$
{\rm cut}(S) = \text{<< number of edges between $S$ and $S^c$ >>}
= \sum_{i\in S, j \in S^c} A_{i,j},
$$
where $A$ denotes the adjacency matrix of $G$. If we encode $S$ in the vector $x$ by
$$
x_i =
\begin{cases}
	1 & \text{if} \ i \in S \\
	-1 & \text{otherwise}
\end{cases}
$$
one can rewrite using \eqref{eq:lap}
$$
{\rm cut}(S) = \frac{1}{4} \sum_{i \sim j} (x_i-x_j)^2
=\frac{1}{4} x^{\sT} L x.
$$
However, ${\rm cut}(S)$ is minimal for $S = \emptyset$ or $S = \{1, \dots, n\}$. Hence the cut is not a good metric, one should add some constraints on $S$.
We can for instance force $S$ to be balanced ($\# S = \# S^c = n/2$, assuming here that $n$ is even) which is equivalent to say that $x$ is orthogonal to $\1$ the vector of all ones. Thus
\begin{center}
	minimize ${\rm cut}(S)$ subject to $S \subset \{1, \dots, S\}$, $S$ balanced,
\end{center}
is equivalent to
\begin{center}
	minimize $x^{\sT} L x$ subject to $x \in \{-1,1\}^n$, $x \perp \1$.
\end{center}
This problem is called the ``minimum bisection'' problem and is known to be NP hard. This comes from the fact that optimizing over the hypercube $\{\pm 1\}^n$ makes the problem combinatorial and difficult to solve. A way around is to relax the constraint by only forcing $x$ to belong on the sphere of radius $\sqrt{n}$ and to solve:
$$
\min_{\|x\| = \sqrt{n}, \ x \perp \1} x^{\sT} L x.
$$
By Proposition \ref{prop:eigen_var} from Lecture~7, we know that the minimum is achieved by the Fiedler eigenvector $v_2$ (recall that $v_1 = \1$ is the first eigenvector of the Laplacian).

However $v_2$ may not belong to $\{\pm 1\}^n$. So in order to obtain a partition $(S,S^c)$ from $v_2$, we finally perform a ``rounding procedure'':
$$
S = \{ i \, | \, v_2(i) \geq 0\}.
$$
This is exactly the spectral clustering studied in the previous sections.

\section{Spectral clustering beyond graphs}

A natural generalization is to consider weighted graphs, where each edge has a weight indicating how close two connected neighbors are.
This amounts to allow the entries of the adjacency matrix $A$ to take different values from $0$ and $1$: $A_{i,j}$ quantify how ``close'' $i$ and $j$ are.
We call such matrices ``similarity matrices''.

We can straightforwardly extend all the object defined above in the particular case of adjacency matrices to the case of similarity matrices. For instance $\deg(i) = \sum_{j} S_{i,j}$.


\section*{Further reading}

The very interesting paper \cite{ng2002spectral} uses the (normalized) Laplacian to cluster points in $\R^d$.
See \cite{von2007tutorial} for a very nice introduction to spectral clustering and \cite{spielman2012spectral} for lecture notes on spectral graph theory.


\vspace{1cm}
\centerline{\pgfornament[width=7cm]{71}}


\bibliographystyle{plain}
\bibliography{../references.bib}
\end{document}

\section{Recitation 6}
It may be helpful if after recitation you try to re-solve these
problems by yourself, and use them as additional study problems for
the class.
\begin{enumerate}
\item When solving the least squares problem
  $$\min_x\|Ax-b\|^2$$
  we obtained the normal equations
  $$A^TAx = A^Tb.$$
  \begin{enumerate}
  \item Under what conditions is $A^TA$ invertible?
  \item If $A^TA$ is not invertible, must the normal equations still
    have a solution?
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item $A^TA$ is invertible when $\ker(A)=0$ (which is equivalent to
    saying the columns of $A$ are linearly independent).  Note, this
    is also exactly when $A$ has a left inverse, and one such left
    inverse is given by the pseudoinverse
    $$A^+ = (A^TA)^{-1}A^T.$$
  \item Yes.  
    Note that $A^TAx=A^Tb$ has a solution exactly when
    $A^Tb\in\im(A^TA)$.  But $\im(A^TA)=\im(A^T)$ (follows from the 
    solutions to homework questions~3.3 and~3.4), so we obtain the result.

    For a quick sketch of why $\im(A^TA)=\im(A^T)$ first note that
    $\im(A^TA)\subseteq\im(A^T)$ is relatively easy to see.  Furthermore,
    $\ker(A^TA)=\ker(A)$ since $Ax=0$ if and only if $x^TA^TAx=0$.
    This implies $\rank(A^TA)=\rank(A)$ by the fundamental theorem.
    But we know $\rank(A)=\rank(A^T)$ so $\dim\im(A^TA)=\dim\im(A^T)$
    completing the sketch.
  \end{enumerate}
\end{solution}
\item Let $b\in\RR^n$, let $A\in\RR^{n\times 2}$ have the form
  $$A = \MAT{1 & t_1\\1 & t_2\\\vdots & \vdots\\1&t_n},$$
  and let $x^*\in\RR^2$ be a minimizer for the least squares problem
  $$\min_x\|Ax-b\|^2.$$
  Prove that $\sum_{i=1}^n (Ax^*-b)_i=0$. [Note: In linear regression
    terminology, this is the statement that the residuals have zero
    mean.]
\begin{solution}
\item[]\Sol Since $Ax^*=P_{\im(A)}(b)$ we have $Ax-b\perp\im(A)$.  But
  the all 1's vector $\mathbf{1}$ is in $\im(A)$, so
  $$0 = \mathbf{1}^T(Ax^*-b) = \sum_{i=1}^n (Ax^*-b)_i.$$
\end{solution}
\item For any $\alpha\in\RR^3$ define the function
  $f_{\alpha}:\RR\to\RR$ by
  $f_\alpha(x)=\alpha_1+\alpha_2x+\alpha_3x^2$.  Given a dataset
  $(t_1,y_1),\ldots,(t_n,y_n)\in\RR^2$ show how to solve the least
  squares problem
  $$\min_\alpha \sum_{i=1}^n (f_\alpha(t_i)-y_i)^2.$$
\begin{solution}
\item[]\Sol Define a matrix $A\in\RR^{n\times 3}$ by
  $$A = \MAT{1 & t_1 & t_1^2\\ 1 & t_2 & t_2^2
  \\ \vdots&\vdots&\vdots\\ 1 & t_n & t_n^2}.$$
  Then we have
  $$\sum_{i=1}^n (f_\alpha(t_i)-y_i)^2 = \|A\alpha-y\|^2,$$
  where $y\in\RR^n$ has $y_i$ as its $i$th component.  Thus we can
  obtain the solution by solving the normal equations
  $$A^TA\alpha = A^Ty$$
  for $\alpha$.
\end{solution}
\item Let $A\in\RR^{m\times n}$ have linearly independent columns.
  \begin{enumerate}
  \item Give an expression for $P_{\im(A)}$ in terms of the
    $QR$-factorization of $A$.
  \item Give an expression for $P_{\im(A)}$ in terms of $A$.
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item The $QR$ factorization lets us write $A=QR$ where $Q$ has
    orthonormal columns and $R$ is upper triangular.  Here the columns
    of $Q$ form an orthonormal basis for $\im(A)$ so we have
    $P_{\im(A)}=QQ^T$ by the previous lab.
  \item If $x^*$ is a minimizer for $\|Ax-b\|^2$ then
    $Ax^*=P_{\im(A)}(b)$.  Thus we have
    $$P_{\im(A)} = A(A^TA)^{-1}A^T=AA^+.$$
  \end{enumerate}
\end{solution}
\item If $Q$ is orthogonal prove that  $\langle x,y\rangle = \langle
  Qx,Qy\rangle$ for all $x,y\in\RR^n$.
\begin{solution}
\item[]\Sol Note that
  $$\langle Qx,Qy\rangle = (Qx)^T(Qy) = x^TQ^TQy = x^Ty=\langle x,y\rangle.$$
\end{solution}
\item Let $U$ be a subspace of $\RR^n$.  Show that
  $\dim(U)+\dim(U^\perp)=n$.
\begin{solution}
\item[]\Sol Let $v_1,\ldots,v_k$ be a basis for $U$ and let
  $w_1,\ldots,w_p$ be a basis for $U^\perp$.  We claim that
  $v_1,\ldots,v_k,w_1,\ldots,w_p$ is a basis for $\RR^n$ (so that
  $k+p=n$).  First note that the list is linearly independent by a
  result from lab~2 since $U\cap U^\perp =\{0\}$: if $v\in
  U\cap U^{\perp}$ then $\langle v,v\rangle = 0$ so $v=0$.  We know
  that our list spans $\RR^n$ since homework 5 shows that any vector
  $u\in\RR^n$ can be written as $u=x+y$ where $x\in U$ and $y\in U^\perp$.
\end{solution}
\item Consider the block matrices
  $$J = \MAT{A & B}\in\RR^{m\times(n+k)}\quad\text{and}\quad Q =
  \MAT{C\\D}\in\RR^{(n+k)\times p}$$
  where $A\in\RR^{m\times n}$, $B\in\RR^{m\times k}$,
  $C\in\RR^{n\times p}$ and $D\in\RR^{k\times p}$.  Compute $JQ$ and
  $Qx$ where $x\in\RR^p$.
\begin{solution}
\item[]\Sol $JQ = AC+BD$ and $Qx=\MAT{Cx\\Dx}$.
\end{solution}
\item Let $f,g:[0,1]\to\RR$ be defined by $f(x)=1$ and $g(x)=x$.
   Recall that the continuous functions from $[0,1]$ to
  $\RR$ form a vector space with inner product
  $$\langle p,q\rangle = \int_0^1 p(t)q(t)\,dt.$$
  \begin{enumerate}
  \item Show that $f,g$ are linearly independent.
  \item We just showed that $f,g$ is a basis for $\Span(f,g)$.  Call
    this basis $B$.  Define the \textit{coordinatization} function
    $\Phi_B:\Span(f,g)\to\RR^3$ by
    $$\Phi_B(\alpha_1f + \alpha_2g) = \alpha.$$
    That is, it maps a vector in $\Span(f,g)$ to its vector of
    coefficients.
    Compute $\Phi_B(j)$ where $j(x)=4x-3$.
  \item Compute an orthonormal basis $C$ for $\Span(f,g)$.
  \item Compute $\Phi_C(j)$ where $j(x)=4x-3$.
  \item Let $k(x)=x^2$.  Compute $P_{\Span(f,g)}(k)$.
  \item In what sense is $P_{\Span(f,g)}(k)$ the best approximation of $k$.
  \end{enumerate}
\begin{solution}
  \item[]\Sol
  \begin{enumerate}
  \item Note that $g$ is not constant, so it is not a scalar multiple
    of $f$.
  \item $\Phi_B(j)=(-3,4)$.
  \item We apply Gram-Schmidt.  Let $u_1=f$ since $\|f\|=1$.  Define
    $w_2:[0,1]\to\RR$ by
    $$w_2 = g - \langle g,u_1\rangle u_1 = x -\int_0^1t\,dt = x-1/2.$$
    Then we set $u_2=w_2/\|w_2\|$ where
    $$\|w_2\|^2 = \int_0^1 (x-1/2)^2\,dx = \int_{-1/2}^{1/2}u^2\,du =
    \frac{1}{12}$$
    so $u_2=\sqrt{12}(x-1/2)$.
  \item $\Phi_C(j) = (-1,4/\sqrt{12})$
  \item Recall that
    $$P_{\Span(f,g)}(k)= \langle k,u_1\rangle u_1 + \langle
    k,u_2\rangle u_2.$$
    Computing we have
    $$\begin{array}{rcl}
      \langle k,u_1\rangle & = & \int_0^1 x^2\,dx = 1/3\\
      \langle k,u_2\rangle & = & \sqrt{12}\int_0^1 x^3-x^2/2\,dx =
      \sqrt{12}(1/4-1/6) = \frac{1}{\sqrt{12}}.
    \end{array}$$
    Thus
    $$P_{\Span(f,g)}(k)= 1/3 + (x-1/2) = x-1/6.$$
  \item By definition, $P_{\Span(f,g)}(k)$ is the minimizer of
    $$\min_{h\in\Span(f,g)}\|h-k\|^2$$
    where
    $$\|h-k\|^2 = \int_0^1 (h(x)-k(x))^2\,dx.$$
  \end{enumerate}
\end{solution}
\item We can also use coordinatization for $\RR^n$.
  If we have a basis $B=v_1,\ldots,v_n$ for $\RR^n$
  then we can define the coordinatization (or change-of-basis) map
  $\Phi_B:\RR^n\to\RR^n$ by
  $$\Phi_B(\alpha_1v_1+\cdots+\alpha_nv_n)=\alpha.$$
  \begin{enumerate}
  \item Let $B$ denote the basis $(1,0),(-1,1)$ for $\RR^2$. Compute
    $$\Phi_B\left(\MAT{1\\0}\right),\quad
    \Phi_B\left(\MAT{-1\\1}\right),\quad \text{and}\quad
    \Phi_B\left(\MAT{0\\1}\right).$$
  \item Suppose $B=v_1,\ldots,v_n$ is a basis for $\RR^n$.
    Give the matrices corresponding to $\Phi_B$ and $\Phi_B^{-1}$
    (possible since $\Phi_B:\RR^n\to\RR^n$ is linear and invertible).
  \item For which bases $B$ of $\RR^n$ does $\Phi_B$ preserve inner products?
    That is, for which bases $B$ does
    $$\langle \Phi_B(x),\Phi_B(y)\rangle = \langle x,y\rangle$$
    for all $x,y\in \RR^n$?
  \end{enumerate}
\begin{solution}
\item[]\Sol
  \begin{enumerate}
  \item
    $$\begin{array}{rcl}
    \Phi_B\left(\MAT{1\\0}\right) & = & \MAT{1\\0}\vspace{0.5cm},\\
    \Phi_B\left(\MAT{-1\\1}\right) & = & \MAT{0\\1}\vspace{0.5cm},\\
    \Phi_B\left(\MAT{0\\1}\right) & = & \MAT{1\\1}.
    \end{array}$$
  \item Let $A\in\RR^{n\times n}$ denote the matrix with $v_i$ as its
    $i$th column.  Then $\Phi_B=A^{-1}$ and $\Phi_B^{-1}=A$.
  \item Orthonormal bases (see question 5 above).
  \end{enumerate}
\end{solution}
\end{enumerate}

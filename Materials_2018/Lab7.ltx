\section{Recitation 7}
It may be helpful if after recitation you try to re-solve these
problems by yourself, and use them as additional study problems for
the class.
\begin{enumerate}
\item Let $A\in\RR^{m\times n}$, $B\in\RR^{m\times m}$ and
  $C\in\RR^{n\times n}$.  Prove that if $B$ and $C$ are invertible
  then $\rank(A)=\rank(BAC)$.  What does this say about the spectral
  decomposition?
\begin{solution}
\item[]\Sol By the homework we know that $\rank(AC)\leq \rank(A)$.
  Let $v\in\im(A)$ so that $v=Ax$ for some $x\in\RR^n$.  Then
  $v=AC(C^{-1}x)$ so $v\in\im(AC)$.  Thus $\rank(AC)=\rank(A)$.  For
  the other side, note that
  $$\rank(BA)=\rank(A^TB^T)=\rank(A^T)=\rank(A)$$
  by applying the previous argument, and
  noting that $B^T$ is invertible (since
  $(B^{-1})^TB^T=(BB^{-1})^T=I^T$).
  Thus
  $$\rank(BAC) = \rank((BA)C)=\rank(BA)=\rank(A).$$
  This proves that the rank of a symmetric matrix is equal to its
  number of non-zero eigenvalues.  To see this suppose the spectral
  decomposition of $M$ is given by $M=V\Lambda V^T$.  Then we have
  $$\rank(M)=\rank(V\Lambda V^T) = \rank(\Lambda)$$
  since $V,V^T$ are orthogonal and thus invertible.
\end{solution}
\item Suppose $D\in\RR^{n\times n}$ is diagonal.
  Give a vector $v\in\RR^n$ with $\|v\|=1$ such that $\|Dv\|$
  maximized.
\begin{solution}
\item[]\Sol Note that
  $$\|Dv\|^2 = \sum_{i=1}^n (D_{ii}v_i)^2 \leq \left(\max_i
  D_{ii}^2\right)\sum_{i=1}^n v_i^2 = \max_i D_{ii}^2.$$
  Thus we can choose $v=e_{j}$ where $|D_{jj}|$ is the largest
  absolute diagonal entry of $D$.
\end{solution}
\item Suppose $A\in\RR^{n\times n}$ is symmetric.
  Give a vector $v$ with $\|v\|=1$ such that $\|Av\|$ maximized.
\begin{solution}
\item[]\Sol By the spectral theorem we have $A=U\Lambda U^T$ where
  $\Lambda$ is
  diagonal and $U$ is orthogonal.  Write $v$ as
  $$v = \alpha_1u_1+\cdots+\alpha_nu_n$$
  where $u_1,\ldots,u_n$ are the columns of $U$.  Then we have
  $$\|Av\|^2 = \|U\Lambda U^T\sum_{i=1}^n \alpha_iu_i\|^2
  = \|\sum_{i=1}^n \alpha_i\lambda_iu_i\|^2 =
  \sum_{i=1}^n \alpha_i^2\lambda_i^2\leq
  \max_i\lambda_i^2\sum_{i=1}^n \alpha_i^2 = \max_i\lambda_i^2$$
  where $\lambda_i=\Lambda_{ii}$.  Thus
  we can choose $v=u_j$ where $|\lambda_j|$ is the largest absolute eigenvalue.
\end{solution}
\item Suppose $A\in\RR^{m\times n}$.
  Give a vector $w$ with $\|w\|=1$ such that $\|Aw\|$ maximized.
\begin{solution}
\item[]\Sol See extra credit 7.5 on the homework.
\end{solution}
\item Let $A\in\RR^{n\times n}$ have eigenvalue $\lambda$.  Prove that
  $$E_\lambda = \{v \in\RR^n : Av = \lambda v\}$$
  is a subspace of $\RR^n$ (called the eigenspace of $A$ corresponding to $\lambda$).
\begin{solution}
\item[]\Sol
  \begin{itemize}
  \item $A0=0=\lambda\cdot 0$ so $0\in E_\lambda$.
  \item If $v,w\in E_\lambda$ then
    $$A(v+w)=Av+Aw=\lambda v+\lambda w = \lambda(v+w)$$
    proving $v+w\in E_\lambda$.
  \item If $v\in E_\lambda$ and $c\in\RR$ then
    $$A(cv) = cAv = c\lambda v = \lambda(cv)$$
    proving $cv\in E_\lambda$.
  \end{itemize}
\end{solution}
\item Let $A\in\RR^{n\times n}$ have eigenvalue $\lambda$.  How would
  you find a non-zero vector $v\in\RR^n$ such that $Av=\lambda v$?
\begin{solution}
\item[]\Sol Solve the linear system $(A-\lambda I)v=0$.
\end{solution}
\item Let $A\in\RR^{m\times n}$ and let $k=\min(m,n)$.
  Show there are orthogonal matrices
  $U\in\RR^{m\times m}$ and $V\in\RR^{n\times n}$ such that $A=U\Sigma
  V^T$ where $\Sigma\in\RR^{m\times n}$ is a diagonal (rectangular) matrix with
  non-negative entries (in other words, $\Sigma_{ij}=0$ if $i\neq
  j$).  The diagonal entries are labeled
  $\sigma_1=\Sigma_{11},\ldots,\sigma_k=\Sigma_{kk}$ and are ordered
  so that $\sigma_1\geq\cdots\geq\sigma_k$.
  This is called the singular value decomposition (SVD) of $A$, the
  values $\sigma_1,\ldots,\sigma_k$ are called the singular values of
  $A$, the columns of $U$ are called the left singular vectors of $A$,
  and the columns of $V$ are called the right singular vectors of $A$.
\begin{solution}
\item[]\Sol
  \begin{proof}
    Note that $A^TA\in\RR^{n\times n}$ is symmetric, so we can apply
    the spectral theorem to obtain
    $$A^TA = V\Lambda V^T$$
    where $V\in\RR^{n\times n}$ is orthogonal and
    $\Lambda\in\RR^{n\times n}$ is diagonal.  Suppose $A^TA$ has rank
    $r$ and we order the columns of $V$ so that $\lambda_{r+1}=0$,
    \ldots, $\lambda_{n}=0$.
    Let $w_1,\ldots,w_n$ denote the $n$ columns of $AV$.  We claim
    that $w_1,\ldots,w_n$ are orthogonal (but not necessarily orthonormal).
    To see this note that
    $$V^TA^TAV = \Lambda,$$
    which is diagonal.  This proves the claim since $\Lambda_{ij}=w_i^Tw_j$.
    Note also that $w_{r+1},\ldots,w_n$ are zero,
    since their squared lengths are given by
    $\lambda_{r+1},\ldots,\lambda_n$, which are assumed to be zero.
    Let $u_i=w_i/\|w_i\|$ for $i=1,\ldots,r$, and extend with $m-r$
    new vectors to form an orthonormal basis
    $$u_1,\ldots,u_r,u_{r+1},\ldots,u_m.$$
    Then we have
    $$AV = U\Sigma$$
    where $U\in\RR^{m\times m}$ has $u_i$ as its $i$th column and
    $\Sigma\in\RR^{m\times n}$ is given by
    $$\Sigma = \MAT{\|w_1\|\\&\ddots\\&&\|w_r\|\\&&&0\\&&&&\ddots},$$
    with zeros on the off-diagonal.  Thus $A=U\Sigma V^T$ as required.
  \end{proof}
\end{solution}
\item Let $A\in\RR^{m\times n}$.  Give a method for computing
  $\rank(A)$ using the SVD of $A$.
\begin{solution}
\item[]\Sol Writing $A=U\Sigma V^T$ we can simply count the number of
  non-zero entries in $\Sigma$ since $U,V$ are invertible.
\end{solution}
\item Explain the following statement: For any $A\in\RR^{m\times n}$,
  the set $\{Ax : \|x\|= 1\}$ is an ellipsoid.  In other words, the
  image of the sphere under a linear transformation is always an
  ellipsoid.
\begin{solution}
\item[]\Sol Using the SVD write $A=U\Sigma V^T$.  $V^T$ is orthogonal,
  so it preserves lengths and maps the sphere $\{x:\|x\|=1\}$ to itself.
  Then $\Sigma$ stretches the sphere along
  each axis creating an ellipsoid.  Finally $U$ is orthogonal, so it
  rotates the ellipsoid.
\end{solution}
\end{enumerate}

\section{Recitation 3}
\begin{enumerate}
\item Let $x\in\RR^n$ and $y\in\RR^m$ be treated as column vectors
  (which is standard).  How many rows and columns does $xy^T$ have?
  What is its rank?
\begin{solution}
\item[]\Sol $xy^T\in\RR^{n\times m}$.  It will have rank $0$ if $x=0$
  or $y=0$
  and rank $1$ otherwise.  To see this, note that the $i$th column is
  $y_ix$, so assuming $y\neq 0$ we have $\Span(x)=\Im(xy^T)$.
\end{solution}
\item Let $x,y\in\RR^n$ be treated as column vectors.  How many rows
  and columns does $x^Ty$ have?  What are its entries?
\begin{solution}
\item[]\Sol It has 1 row and 1 column (i.e., a real number).  The
  value is given by
  $$x^Ty = x_1y_1+\cdots+x_ny_n.$$ 
\end{solution}
\item Let $A\in\RR^{m\times n}$. Show that if $x\in\Ker(A)$ then
  $v_i^Tx=0$ for $i=1,\ldots,m$ where $v_i^T\in\RR^{1\times n}$ is the $i$th row of $A$.
\begin{solution}
\item[]\Sol
  \begin{proof}
    By the formula for matrix multiplication we have
    $$0=Ax =
    \MAT{\horzbar&v_1^T&\horzbar\\\horzbar&v_2^T&\horzbar\\&\vdots\\\horzbar&v_m^T&\horzbar}
    \MAT{\vertbar\\x\\\vertbar}
    = \MAT{v_1^Tx\\v_2^Tx\\\vdots\\v_m^Tx}.$$
  \end{proof}
\end{solution}
\item Fix $A\in\RR^{4\times 5}$ and describe the following set:
  $$\left\{Ax : x=\MAT{a\\b\\0\\0\\c},~a,b,c\in\RR\right\}.$$
\begin{solution}
\item[]\Sol The span of the first, second, and fifth columns of $A$.
\end{solution}
\item Given a matrix $A\in\RR^{3\times 3}$, show how to swap the first
  2 rows via matrix multiplication.
\begin{solution}
\item[]\Sol
  $$\MAT{0 & 1 & 0\\1 & 0 & 0\\0 & 0 & 1}A.$$
\end{solution}
\item Given a matrix $A\in\RR^{3\times 3}$, show how to replace the
  first row with the sum of the first row and $5$ times the third row
  via matrix multiplication.
\begin{solution}
\item[]\Sol
  $$\MAT{1 & 0 & 5\\0 & 1 & 0\\0 & 0 & 1}A.$$
\end{solution}
\item Given a matrix $A\in\RR^{3\times 3}$, show how to swap the first
  2 columns via matrix multiplication.
\begin{solution}
\item[]\Sol
  $$A\MAT{0 & 1 & 0\\1 & 0 & 0\\0 & 0 & 1}.$$
\end{solution}
\item $(\star)$ Let $A\in\RR^{m\times k}$ and let $B\in\RR^{k\times n}$.  Show
  that $AB$ can be written as
  $$AB = C_1+C_2+\cdots+C_k$$
  with $\rank(C_i)\leq1$ for $i=1,\ldots,k$.
\begin{solution}
\item[]\Sol Let $a_i$ denote the $i$th column of $A$ and let $b_j^T$
  denote the $j$th row of $B$.  Then we have
  $$AB =
  \MAT{
    \vertbar&\vertbar&&\vertbar\\
    a_1&a_2&\cdots&a_k\\
    \vertbar&\vertbar&&\vertbar
  }
  \MAT{
    \horzbar&b_1^T&\horzbar\\
    \horzbar&b_2^T&\horzbar\\
    &\vdots&\\
    \horzbar&b_k^T&\horzbar\\
  }
  = a_1b_1^T + a_2b_2^T+\cdots + a_kb_k^T.$$
\end{solution}
\item (Fundamental Theorem) ($\star$) Let $L:\RR^n\to\RR^m$ be linear.  Prove
  that $\dim(\Ker(L))+\rank(L)=n$.
\begin{solution}
\item[]\Sol
  \begin{proof}
    Let $v_1,\ldots,v_p\in\RR^n$ be a basis for $\Ker(L)$, where $p=\dim(\Ker(L))$.  By
    repeatedly adding vectors not in the span (previous lab) we can
    grow this list to form a basis for $\RR^n$:
    $$v_1,\ldots,v_p,v_{p+1},\ldots,v_n.$$
    We claim that $Lv_{p+1},\ldots,Lv_n$ form a basis for $\Im(L)$.
    Note that if we can show this claim we are done, since this gives
    $$\rank(L)=\dim(\Im(L))=n-p.$$

    We first show that we have a spanning set.  Let $w\in\RR^n$ so
    that $Lw$ is an arbitrary element of $\Im(L)$.  Since
    $v_1,\ldots,v_n$ is a basis for $\RR^n$ we can write
    $$w = \alpha_1v_1+\cdots+\alpha_nv_n.$$
    But then
    $$Lw = \alpha_1Lv_1+\cdots+\alpha_nLv_n
    = \alpha_{p+1}Lv_{p+1}+\cdots+\alpha_nLv_n$$
    since $v_1,\ldots,v_p\in\Ker(L)$.  Thus $Lw\in\Span(Lv_{p+1},\ldots,Lv_{n})$.

    Next we show that we have a linearly independent set.  Suppose
    $$\alpha_{1}Lv_{p+1}+\cdots+\alpha_{n-p}Lv_n=0$$
    for some $\alpha\in\RR^{n-p}$.  Then
    $$L(\alpha_1v_{p+1}+\cdots+\alpha_{n-p}v_n)=0$$
    by linearity.  If $\alpha_1v_{p+1}+\cdots+\alpha_{n-p}v_n\neq0$
    then it is a non-zero vector in the kernel, so we can write
    $$\alpha_1v_{p+1}+\cdots+\alpha_{n-p}v_n =
    \beta_1v_1+\cdots+\beta_pv_p$$
    showing that $v_1,\ldots,v_n$ are linearly dependent, a contradiction.
  \end{proof}
\end{solution}
\item ($\star$) Let $L:\RR^n\to\RR^m$ be linear.  Show that every $x\in\RR^n$
  can be written uniquely as $x=u+v$ where $u\in\Ker(L)$ and
  $v\in\Im(L^T)$ (also called the row space).
\begin{solution}
\item[]\Sol
  \begin{proof}
    Let $v_1,\ldots,v_p$ be a basis for $\Ker(L)$ with $p=\dim\Ker(L)$
    and let $w_1,\ldots,w_q$ be a basis for $\Im(L^T)$ (the row space)
    with $q=\rank(L)$ (homework shows $\rank(L)=\rank(L^T)$).  We
    claim that
    $$v_1,\ldots,v_p,w_1,\ldots,w_q$$
    is a basis for $\RR^n$.  Note that $p+q=n$ by the fundamental
    theorem, so we need only prove linear independence.  By the
    previous lab, this reduces to proving that
    $$\Ker(L)\cap \Im(L^T)=\{0\}.$$
    To that end, let $u\in \Ker(L)\cap\Im(L^T)$.  Since $u\in\Im(L^T)$
    we can write
    $$u = \alpha_1x_1+\cdots+\alpha_mx_m$$
    where $x_1^T,\ldots,x_m^T$ are the rows of the matrix
    corresponding to $L$.  Note that
    $$u^Tu = (\alpha_1x_1+\cdots+\alpha_mx_m)^Tu
    = \alpha_1x_1^Tu+\cdots+\alpha_mx_m^Tu = 0+\cdots+0=0$$
    since $u\in\Ker(L)$ (apply exercise 3 above).  But note
    that
    $$0=u^Tu = u_1^2+\cdots+u_n^2$$
    implies $u_1=u_2=\cdots=u_n=0$.
  \end{proof}
\end{solution}
\item Determine all solutions $x\in\RR^3$ to
  $$\MAT{1&3&2\\2&6&5\\1&3&3}x = \MAT{0\\0\\0}.$$
\begin{solution}
\item[]\Sol
  Suppose $x$ is a solution.  By multiplying both sides of the
  equation by the appropriate matrices (see earlier exercises) we can
  row-reduce the matrix while maintaining the equality.  This gives
  $$\begin{array}{rcll}
    \MAT{1&3&2\\2&6&5\\1&3&3}x & = & \MAT{0\\0\\0} &
    \implies    \vspace{0.5cm}
    \\\vspace{0.5cm}
    \MAT{1&3&2\\0&0&1\\1&3&3}x & = & \MAT{0\\0\\0} & \implies\\\vspace{0.5cm}
    \MAT{1&3&2\\0&0&1\\0&0&1}x & = & \MAT{0\\0\\0} &
    \implies\\\vspace{0.5cm}
    \MAT{1&3&2\\0&0&1\\0&0&0}x & = & \MAT{0\\0\\0}.    
  \end{array}$$
  By noting that each row reduction operation is reversible (the
  correponding matrices are invertible) we see that $x$ is a solution
  to our original equation iff $x$ is a solution to our final row
  reduced equation.  Writing the above in terms of the coordinates of
  $x=(x_1,x_2,x_3)$ we have
  $$\MAT{x_1+3x_2+2x_3\\x_3\\0}=\MAT{0\\0\\0}.$$
  Solving in terms of $x_2$ (which happens to correspond to a column
  with no leading non-zero coefficient in it) we obtain
  $$\begin{array}{rcl}
    x_1 & = & -3x_2 \\
    x_3 & = & 0.
  \end{array}$$
  Thus the solution set is (letting $c$ be a placeholder for the value
  of $x_2$)
  $$\left\{\MAT{-3c\\c\\0} : c\in\RR\right\}
  = \left\{c\MAT{-3\\1\\0} : c\in\RR\right\}
  = \Span\left(\MAT{-3\\1\\0}\right).$$
\end{solution}
\end{enumerate}
